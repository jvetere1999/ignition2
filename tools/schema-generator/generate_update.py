"""
Incremental Schema Migration Generator

Compares current schema against master_schema and generates incremental migrations.
No entry point - designed to be called by other tools in the future.

Status: Library implementation (non-intrusive)
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional
from datetime import datetime
from copy import deepcopy
import hashlib


class SchemaDiffer:
    """Detect differences between two schema versions."""

    @staticmethod
    def diff_schemas(
        master_schema: Dict[str, Any],
        new_schema: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Compare two schemas and return detailed diff.
        
        Returns:
            {
                "tables_added": [...],
                "tables_removed": [...],
                "tables_renamed": [...],
                "tables_modified": {...},
                "change_count": int
            }
        """
        master_tables = set(master_schema.get("tables", {}).keys())
        new_tables = set(new_schema.get("tables", {}).keys())

        tables_added = new_tables - master_tables
        tables_removed = master_tables - new_tables
        tables_unchanged = master_tables & new_tables

        diff = {
            "tables_added": sorted(list(tables_added)),
            "tables_removed": sorted(list(tables_removed)),
            "tables_modified": {},
            "change_count": len(tables_added) + len(tables_removed),
        }

        # Check modified tables
        for table_name in tables_unchanged:
            field_diffs = SchemaDiffer._diff_table_fields(
                master_schema["tables"][table_name],
                new_schema["tables"][table_name],
            )
            if field_diffs["changes"]:
                diff["tables_modified"][table_name] = field_diffs
                diff["change_count"] += len(field_diffs["changes"])

        return diff

    @staticmethod
    def _diff_table_fields(
        master_table: Dict[str, Any],
        new_table: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Detect field-level changes within a table."""
        master_fields = set(master_table.get("fields", {}).keys())
        new_fields = set(new_table.get("fields", {}).keys())

        fields_added = new_fields - master_fields
        fields_removed = master_fields - new_fields
        fields_unchanged = master_fields & new_fields

        changes = []

        # Track added fields
        for field_name in fields_added:
            changes.append({
                "type": "add_column",
                "column": field_name,
                "definition": new_table["fields"][field_name],
            })

        # Track removed fields
        for field_name in fields_removed:
            changes.append({
                "type": "drop_column",
                "column": field_name,
                "definition": master_table["fields"][field_name],
            })

        # Track modified fields
        for field_name in fields_unchanged:
            master_field = master_table["fields"][field_name]
            new_field = new_table["fields"][field_name]
            if master_field != new_field:
                changes.append({
                    "type": "modify_column",
                    "column": field_name,
                    "old_definition": master_field,
                    "new_definition": new_field,
                })

        return {"changes": changes}


class MigrationGenerator:
    """Generate SQL migration statements from schema diffs."""

    @staticmethod
    def generate_migration_sql(
        diff: Dict[str, Any],
        master_schema: Dict[str, Any],
        new_schema: Dict[str, Any]
    ) -> str:
        """
        Generate SQL statements to transform master schema to new schema.
        
        Returns SQL migration file content (ready to write to file).
        """
        statements = [
            "-- Auto-generated incremental migration",
            f"-- Generated at {datetime.utcnow().isoformat()}Z",
            "-- DO NOT EDIT: Generated by generate_update.py",
            "",
        ]

        # Drop removed tables
        for table in diff["tables_removed"]:
            statements.append(f"DROP TABLE IF EXISTS {table} CASCADE;")

        # Add new tables
        for table in diff["tables_added"]:
            statements.append(MigrationGenerator._generate_create_table(
                table, new_schema["tables"][table]
            ))

        # Modify existing tables
        for table_name, field_diffs in diff["tables_modified"].items():
            for change in field_diffs["changes"]:
                sql = MigrationGenerator._generate_field_change(
                    table_name, change, new_schema["tables"][table_name]
                )
                if sql:
                    statements.append(sql)

        return "\n".join(statements) + "\n"

    @staticmethod
    def _generate_create_table(table_name: str, table_def: Dict[str, Any]) -> str:
        """Generate CREATE TABLE statement."""
        fields = table_def.get("fields", {})
        lines = [f"CREATE TABLE {table_name} ("]

        field_stmts = []
        for field_name, field_def in fields.items():
            field_stmts.append(
                MigrationGenerator._generate_column_def(field_name, field_def)
            )

        lines.append(",\n  ".join(field_stmts))
        lines.append(");")
        return "  ".join(lines)

    @staticmethod
    def _generate_column_def(field_name: str, field_def: Dict[str, Any]) -> str:
        """Generate column definition for CREATE TABLE."""
        parts = [field_name, field_def.get("type", "TEXT")]

        if not field_def.get("nullable", True):
            parts.append("NOT NULL")

        if field_def.get("primary"):
            parts.append("PRIMARY KEY")

        if field_def.get("unique"):
            parts.append("UNIQUE")

        if "default" in field_def:
            parts.append(f"DEFAULT {field_def['default']}")

        return "  " + " ".join(parts)

    @staticmethod
    def _generate_field_change(
        table_name: str, change: Dict[str, Any], table_def: Dict[str, Any]
    ) -> Optional[str]:
        """Generate ALTER TABLE statement for field change."""
        if change["type"] == "add_column":
            col_def = MigrationGenerator._generate_column_def(
                change["column"], change["definition"]
            ).strip()
            return f"ALTER TABLE {table_name} ADD COLUMN {col_def};"

        elif change["type"] == "drop_column":
            return f"ALTER TABLE {table_name} DROP COLUMN {change['column']} CASCADE;"

        elif change["type"] == "modify_column":
            old_def = change["old_definition"]
            new_def = change["new_definition"]
            new_type = new_def.get("type", "TEXT")
            return f"ALTER TABLE {table_name} ALTER COLUMN {change['column']} TYPE {new_type};"

        return None


class ChangeLogger:
    """Track and persist schema changes."""

    @staticmethod
    def create_change_entry(
        change_type: str,
        table_name: str,
        details: Dict[str, Any],
        migration_file: str,
        reversible: bool = True
    ) -> Dict[str, Any]:
        """Create a change log entry."""
        change_id = ChangeLogger._generate_change_id()
        return {
            "id": change_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "type": change_type,
            "table": table_name,
            "details": details,
            "migration_file": migration_file,
            "status": "pending",
            "rollback_available": reversible,
        }

    @staticmethod
    def _generate_change_id() -> str:
        """Generate unique change ID (CHG-001, CHG-002, etc.)."""
        timestamp = datetime.utcnow().isoformat()
        hash_obj = hashlib.md5(timestamp.encode())
        sequence = int(hash_obj.hexdigest(), 16) % 10000
        return f"CHG-{sequence:04d}"

    @staticmethod
    def update_changes_log(
        changes_log_path: Path,
        new_changes: List[Dict[str, Any]],
        next_migration_number: int
    ) -> None:
        """Append new changes to log and rotate undo stack."""
        with open(changes_log_path, "r") as f:
            log = json.load(f)

        # Append new changes
        log["changes"].extend(new_changes)
        log["total_changes"] += len(new_changes)
        log["last_updated"] = datetime.utcnow().isoformat() + "Z"

        # Track undoable changes
        undoable = [
            c["id"] for c in new_changes if c.get("rollback_available")
        ]
        log["undoable_changes"].extend(undoable)

        # Keep only last 3 undoable
        if len(log["undoable_changes"]) > 3:
            log["undoable_changes"] = log["undoable_changes"][-3:]

        log["metadata"]["next_migration_number"] = next_migration_number

        with open(changes_log_path, "w") as f:
            json.dump(log, f, indent=2)


class SchemaVersionManager:
    """Manage master schema versions and backups."""

    @staticmethod
    def rotate_backups(master_schema_dir: Path, undo_depth: int = 3) -> None:
        """
        Rotate schema versions, keeping only last N backups.
        
        v2.0.0 → v1.9.9
        v1.9.9 → v1.9.8
        ... (archive oldest)
        """
        versions = sorted([
            f.name for f in master_schema_dir.glob("v*.json")
            if f.name != "v2.0.0.json"
        ])

        # Archive oldest versions beyond undo_depth
        for version_file in versions[:-undo_depth]:
            archive_path = master_schema_dir / "archive" / version_file
            old_path = master_schema_dir / version_file
            if old_path.exists():
                old_path.rename(archive_path)

    @staticmethod
    def update_master_schema(
        master_schema_dir: Path,
        new_schema: Dict[str, Any],
        version: str
    ) -> None:
        """Save new schema as master and rotate backups."""
        master_path = master_schema_dir / "v2.0.0.json"

        # First, rotate existing backups
        SchemaVersionManager.rotate_backups(master_schema_dir)

        # Then update master
        new_schema["_metadata"] = {
            "created_at": datetime.utcnow().isoformat() + "Z",
            "last_migration_applied": f"{version}",
            "version": new_schema.get("version", "2.0.0"),
        }

        with open(master_path, "w") as f:
            json.dump(new_schema, f, indent=2)


def generate_incremental_update(
    new_schema_path: Path,
    master_schema_dir: Path,
    changes_log_path: Path,
    migrations_output_dir: Path,
) -> Dict[str, Any]:
    """
    Generate incremental migrations from schema diff.
    
    LIBRARY FUNCTION - Not an entry point
    
    Args:
        new_schema_path: Path to modified schema.json
        master_schema_dir: Path to master_schema/ directory
        changes_log_path: Path to changes/changes.json
        migrations_output_dir: Path to app/backend/migrations/
    
    Returns:
        {
            "status": "success" | "error",
            "migration_number": int,
            "migration_file": str,
            "changes_count": int,
            "undoable_changes": [str],
            "warnings": [str],
            "errors": [str]
        }
    """
    result = {
        "status": "success",
        "migration_number": None,
        "migration_file": None,
        "changes_count": 0,
        "undoable_changes": [],
        "warnings": [],
        "errors": [],
    }

    try:
        # Load schemas
        with open(new_schema_path, "r") as f:
            new_schema = json.load(f)

        master_path = master_schema_dir / "v2.0.0.json"
        with open(master_path, "r") as f:
            master_schema = json.load(f)

        with open(changes_log_path, "r") as f:
            changes_log = json.load(f)

        # Generate diff
        diff = SchemaDiffer.diff_schemas(master_schema, new_schema)

        if diff["change_count"] == 0:
            result["warnings"].append("No schema changes detected")
            return result

        # Generate migration SQL
        migration_sql = MigrationGenerator.generate_migration_sql(
            diff, master_schema, new_schema
        )

        # Calculate migration number
        next_migration = changes_log["metadata"]["next_migration_number"]
        migration_filename = f"000{next_migration}_incremental_update.sql"

        # Create change entries
        new_changes = []
        for table in diff["tables_added"]:
            new_changes.append(
                ChangeLogger.create_change_entry(
                    "add_table",
                    table,
                    {"fields": list(new_schema["tables"][table].get("fields", {}).keys())},
                    migration_filename,
                    reversible=True
                )
            )

        for table in diff["tables_removed"]:
            new_changes.append(
                ChangeLogger.create_change_entry(
                    "drop_table",
                    table,
                    {"fields": list(master_schema["tables"][table].get("fields", {}).keys())},
                    migration_filename,
                    reversible=False
                )
            )

        for table, field_diffs in diff["tables_modified"].items():
            for change in field_diffs["changes"]:
                reversible = change["type"] in ("add_column", "modify_column")
                new_changes.append(
                    ChangeLogger.create_change_entry(
                        change["type"],
                        table,
                        change,
                        migration_filename,
                        reversible=reversible
                    )
                )

        # Update change log
        ChangeLogger.update_changes_log(
            changes_log_path,
            new_changes,
            next_migration + 1
        )

        # Update master schema
        SchemaVersionManager.update_master_schema(
            master_schema_dir,
            deepcopy(new_schema),
            migration_filename
        )

        # Write migration file
        migration_path = migrations_output_dir / migration_filename
        with open(migration_path, "w") as f:
            f.write(migration_sql)

        result.update({
            "status": "success",
            "migration_number": next_migration,
            "migration_file": migration_filename,
            "changes_count": diff["change_count"],
            "undoable_changes": [c["id"] for c in new_changes if c["rollback_available"]],
        })

    except Exception as e:
        result["status"] = "error"
        result["errors"].append(str(e))

    return result
